%Kapitel der Umsetzung

\chapter{OSINT einer großen Anzahl von Person}  %Name des Kapitels
\label{cha:Informationsbeschaffung einer grossen Anzahl von Person} %Label des Kapitels
Für die \textit{real-world} Simulation eines Phishing-Mail-Angriffs eine Webseiten mit einer großen Menge von personenbezogenen Daten benötigt. 	Hierfür wird manuell nach einer Webseite gesucht, die eine große Menge an personenbezogenen Daten enthält. Diese wird anschließen als Informationsquelle festgelegt. Möglichkeiten, ausgenommen von den bekannten Social Media Seiten, sind die Webseiten FuPa, Xing und LinkedIn.\\


\section{Methoden für OSINT}
	\subsection{Methode für die Suche nach Information}
	In diesem Konzept gibt es keine automatisierte Suche nach Informationen, jedoch eine automatisierte Suche nach internen Links. Diese interne Suche kann mit einem Web Crawler realisiert werden. In Vorbereitung darauf wird der Aufbau der Seite analysiert.\\

	\subsection{Methode zum Auslesen der Information}
	Zum Auslesen einer großen Menge an Daten wird ein Web Scraper erstellt. Dieser könnte für die ausgewählte Webseite hartkodiert werden. Eine Alternative dazu, wäre die Analyse des Webseitentextes, was dem Ansatz \ref{subsec:ErkennenVonInformation} von der Suchfunktion einer ausgewählten Person entsprechen würde.
%TODO BILDER von Webseite einfügen für groben Überblick oder in Umsetztung


\section{Bewertung: OSINT große Anzahl}
Die Suchfunktion für eine große Anzahl von Personen kann \textit{hartkodiert} werden und benötigt dadurch keine Textanalyse, da der Aufbau der Webseite im voraus bekannt ist. Das bedeutet, dass das Programm genau weiß wo welche Information auf einer Webseite steht. Auf der Seite "'\textit{www.fupa.net}"' befindet sich beispielsweise der Name einer Person immer an der gleichen Position einer Tabelle. Das bringt den Vorteil mit sich, dass der Text nicht analysiert werden muss und das Programm genau weiß, was mit diesen Daten gemacht werden muss. Zusätzlich entsteht eine sehr performante Methode zur Auslesung von personenbezogenen Daten.


\section{Aufbau einer Webseite analysieren}

\section{Erstellung eines internen Web Crawlers} %Unterunterkapitel
	\label{sse:}
	Damit die Webseite \textit{www.fupa.net} komplett nach Spielerdaten durchsucht werden kann, wird ein interner Web Crawler benötigt. Dieser wird sich anhand den internen Links, über die ganze Seite hinweg, durchhangeln.\\
	Für die Erstellung eines hartkodierten Web Crawlers muss zuerst einmal der komplette Aufbau einer Webseite bekannt sein. Dies lässt sich einfach mit Hilfe der Entwicklertools in einem Browser durchführen. %TODO Möglicherweise Bild von dem Aufbau der Seite
	
	\subsection{Funktionsweise des Web Crawlers}
	Links mit Spielerinformationen speichern.
	Die Funktionsweise des Web Crawlers besteht darin, dass das Programm auf der Startseite von Fupa.net beginnt nach links zu suchen und diesen folgt.
	\subsection{Probleme bei der Erstellung} %Unterkapitel 3. Ordnung
	\begin{enumerate}
		\item Python hat einen verkürzten und erkennbaren Standard http-Header. Dieser wird von vielen Administratoren geblockt und mit der Fehlermeldung 451 erkennbar gemacht. 451 for legal reason
		\item Honeypots gewollt oder ungewollt, hier Kalender darstellung mit links zu neuen Jahren die eine sehr hohe bis überhaupt keine Begrenzung haben.
		\item Rekursion erreicht schnell die Maximale tiefe von 1500.
		\item Zu langsamer Algorithmus
	\end{enumerate}
	
	
	\subsection{Lösungen}
	\begin{enumerate}
		\item http-Header selber konfigurieren
		\item Links mit möglichen Honeypots nicht beachten
		\item Stack Klasse schreiben damit keine Rekursion benötigt wird
		\item Algorithmus anpassen auf fupa-Webseite
	\end{enumerate}

\section{Auslesen der Webseite durch Hartkodierung}

\section{Datenverwaltung und Speicherung}
%TODO Speicherung kommt in Umsetzung, da es nicht zur Kernfrage gehört.
Für die Speicherung der gewonnen Daten kann eine SQL-Datenbank erstellt werden.
Als Alternative kann eine Datei angelegt werden, bei der alle Daten zu allen Personen gut strukturiert gespeichert werden können. Eine Möglichkeit dafür wäre das Dateiformat \textit{CSV} oder \textit{TXT}.