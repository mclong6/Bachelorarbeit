%Kapitel der Umsetzung

\chapter{Implementierung des OSINTs für eine ausgewählte Person}  %Name des Kapitels
\label{cha:Informationsbeschaffung einer ausgewählten Person} %Label des Kapitels
%TODO Beschreiben was in diesem Kapitel gemacht wird!!
\section{Auswahl der Programmiersprache}
Damit das Programm anhand den Lösungsideen umgesetzt werden kann, ist der erste Schritt die Auswahl der Programmiersprache.
	\subsection{Ziele und Anforderungen}
		Es wird keine Anforderung an die Geschwindigkeit der Sprache gestellt, da beim "'web scraping"' das Internet den zeitlichen Engpass darstellt. Allerdings wäre es von Vorteil, wenn bereits entwickelte Bibliotheken für das OSINT vorhanden sind. Die Eingabe der Information für die Suche kann über eine Konsole oder über eine graphische Benutzeroberfläche möglich sein.
	\subsection{Lösungsideen}
		Für eine web-basierende Anwendung eignet sich eine dynamische Programmsprache, da ein Programm zur Laufzeit erweitert werden kann. Infolgedessen zählen Python und Ruby als mögliche Programmiersprache.
	\subsection{Bewertung der Lösungsideen anhand den Anforderungen}
		Beide Sprachen können Webseiten, welche JavaScript-Code enthalten, laden. Dies ist mit Hilfe eines automatisierten Webbrowsers möglich. Des Weiteren lässt sich die Anwendung durch beide Sprachen, entsprechend den Anforderungen, entwickeln. Es kann sowohl eine Oberflächenanwendung, als auch eine Konsolenanwendung programmiert werden. Zusätzlich bringen beide Sprachen Module mit sich, um die vorgegebenen Ziele umzusetzen.\\ Somit haben beide Programmiersprachen die Voraussetzungen für die Entwicklung der Anwendung. Allerdings bietet Python in diesem Bereich eine größere Community und eignet sich sehr gut für die Bearbeitung von linguistischen Daten. \cite{bird2009natural} Aus diesen Gründen wird die zu erstellende Anwendung mit der Programmiersprache Python entwickelt.

\section{Personensuche im Internet}
	\subsection{Ziele und Anforderungen}
	Ziel ist es Methoden zu entwickeln, welche eine Zielperson auf dem optimalem Weg im Internet suchen. Dazu soll die Suche mit Hilfe der eingegebenen Daten eingegrenzt werden, dass möglichst viele zutreffende Informationen über die gesuchte Person gefunden werden kann. Gleichzeitig sollen Fehlergebnisse vermieden werden. Allerdings dürfen dabei nicht zu viele Informationsquellen ignoriert werden, da sonst wichtige Information verloren gehen könnten.
	
	\subsection{Lösungsideen}
	\label{sec:Suche nach Information}
	Die Art der Personensuche wird abhängig von den eingegeben Daten variiert. Das heißt, dass die eingegebenen Daten über die Zielperson vor der Suche analysiert werden. Abhängig von den Ergebnissen der Analyse wird die Personensuche durchgeführt. Für die Implementierung der Suche werden nachfolgend zwei mögliche Methoden beschrieben.
	
		\subsubsection{Personensuche mit Hilfe einer Suchmaschine}
		\label{subsubsec:PersonensucheMitHilfevonSuchmaschine}
		Bei dieser Methode wird mit Hilfe einer Suchmaschine nach Informationen gesucht. Mögliche Suchmaschinen sind "'Google"' und "'Bing"'. Allerdings muss nicht für jede Suche eine Suchmaschine verwendet werden. Die nachfolgenden Fälle sollen diesen Ansatz verdeutlichen.
		
		Im Fall, dass der Vorname, Nachname und Wohnort der gesuchten Person eingegeben wird, kann mit Hilfe der festgelegten Suchmaschine nach Information gesucht werden. Die von der Suchmaschine vorgeschlagenen Seiten werden anschließend analysiert, ausgelesen und gespeichert. Dadurch können weitere Informationen gewonnen werden. Falls Benutzernamen von anderen Webseiten wie Instagram, Facebook oder ähnliches vorgeschlagen werden, kann somit die Suche mit diesen Daten speziell auf den entsprechenden Seiten erweitert werden.\\
		Ein weiteres Szenario beschreibt der Fall, wenn ein Benutzername der gesuchten Person in das Programm eingegeben wird. Hierbei handelt es sich um einen Benutzernamen von Social-Media-Webseiten wie Facebook, Instagram, LinkedIn, et cetera. \\
		Zuallererst wird hier nach Einträgen auf der entsprechende Webseite zu dem angegebenen Benutzername gesucht. Dadurch können zusätzliche Daten herausgefunden werden. Diese sind bei der weiteren Suche von Vorteil.\\
		Sobald die Webseite mit Hilfe des Nutzernamens durchsucht und ausgewertet wurde, kann die Suche mit einer Suchmaschine und den gewonnen Daten erweitert werden.
		
		\subsubsection{Personensuche auf festgelegten Webseiten}
		\label{subsubsec:PersonensucheohneSuchmaschine}
		Unabhängig von den eingegebenen Daten wird eine festgesetzte Anzahl von Webseiten durchsucht. Als potentielle Kandidaten-Webseiten eigenen sich die Social-Media-Seiten wie Facebook, Instagram, Twitter, LinkedIn, et cetera. Diese Art der Personensuche arbeitet allerdings ohne die Verwendung einer Suchmaschine.
		
		\subsection{Bewertung der Lösungsideen anhand den Anforderungen}
		Um möglichst viele zutreffende Informationen über eine Person im Internet zu finden, bietet die Personensuche mit der Verwendung einer Suchmaschine die beste Lösung. Das hat den Grund, dass das ganze Internet nach Informationen durchsucht wird, anstatt ausschließlich auf festgelegten Webseiten. Dadurch können wesentlich mehr individuelle Einträge gefunden werden. Des Weiteren wird keine Logik zur Suche nach Einträgen im Internet benötigt, da lediglich den vorgeschlagenen Suchergebnissen gefolgt werden kann.\\
		Allerdings muss beachtet werden, dass Benutzer bei verschiedensten Social-Media-Seiten auswählen können, ob das Benutzerprofil von einer Suchmaschine gefunden werden kann oder nicht. Bekannte Webseiten die diese Einstellungsmöglichkeiten unterstützen sind XING und LinkedIn. Aus diesem Grund werden zu Beginn der Suche die Social-Media-Seiten durchsucht. Dadurch können vor der Suche mit einer Suchmaschine zusätzliche Informationen herausgefunden werden, die für das spätere OSINT von Vorteil sind. Falls sich eine Social-Media-Seite unter den Suchergebnissen befindet, kann diese nachträglich ebenfalls durchsucht werden.
		
			\subsubsection{Auswahl der Suchmaschine}
				\paragraph{Ziele und Anforderungen}
				Es soll eine Suchmaschine ausgewählt werden, welche einen großen Marktanteil in Deutschland erweist. Dadurch sollen konkrete Suchanfragen möglich sein. 
				\paragraph{Lösungsideen}
				Eine Möglichkeit zur Auswahl ist die Suchmaschine Google von "'Google LLC"'. Eine Alternative hierfür stellt die Suchmaschine Bing dar.
				\paragraph{Bewertung der Lösungsideen anhand den Anforderungen}
				Laut einer Expertenaussage sucht Bing tiefgreifender nach Information auf Social-Media-Plattformen wie Facebook, Twitter und LinkedIn. \cite{Suchmaschinen} Allerdings finden nur 3,5\% aller Suchanfragen in Deutschland über Bing statt. Im Gegensatz dazu hat Google einen Marktanteil von 91,2\% in Deutschland. \cite{Suchmaschinen}\\
				Diese Zahlen sprechen eindeutig für Google. Durch die höhere Anzahl von Suchanfragen, können mehr Daten erfasst und die Ergebnislisten besser bewertet werden. Dies hat zu Folge, dass Bing bei einer konkreten Suche schlechter abschneidet. \\
				Grundsätzlich stellt die Verwendung von zwei Suchmaschinen die beste Lösung dar, da die Wahrscheinlichkeit für einen Suchtreffer erhöht wird. Dennoch wird in dieser Arbeit ausschließlich die Suchmaschine von Google verwendet, da sie gegenüber dem Konkurrenten keine Nachteile hat. Selbst die detailliertere Suche auf Sozialen Netzwerken bringt bei der hier verwendeten Personensuche keinen Vorteil für Bing. Das hat den Grund, dass bei der verwendeten Suche standardmäßig die bekannten Social-Media-Plattformen nach Daten kontrolliert werden. 
		 	
\section{Umsetzung der Personensuche mit Hilfe der Google-Suchmaschine}
Die Suchmaschine von "'Google LLC"' wird für die Personensuche im Internet verwendet. Gesucht wird mit den eingegebenen Daten, welche über die Konsole eingelesen werden.

	\subsection{Eingabe der bekannten Daten}
	Es besteht die Möglichkeit den \textbf{Vorname, Nachname, Geburtsjahr, Geschlecht, Wohnort} bzw. \textbf{Standort , Arbeitgeber, Instagram-Benutzername, Facebook-Benutzername, Twitter-Benutzername} und die \textbf{E-Mail-Adresse} der gesuchten Person über eine Konsole einzugeben.\\
	Zu Beginn werden alle Personenattribute mit einem leeren String initialisiert. Das bedeutet, alle Variablen, zu denen keine Information eingegeben wurde, enthalten einen leeren String.
	
		\subsubsection{Verarbeitung der Daten}
		Im ersten Schritt wird kontrolliert, welche Informationen von dem Progamm-Nutzer eingegeben wurden. Der Vorname und Nachname sind nicht ausreichend für die Suche. Es wird mindestens ein weiteres Attribut benötigt. Dagegen ist der Benutzername von Instagram und Twitter sowie die E-Mail-Adresse einzigartig. Dadurch kann mit einem dieser Attribute gesucht werden.\\
		Bei der Eingabe des Wohnortes, kann dieser vor der Suche mit der entsprechenden Wortsammlung verglichen werden. Falls sich der Wohnort nicht in der Datenbank befindet, wird er nachträglich ergänzt. Das hat den Grund, dass es für die Personenerkennung wichtig ist, dass sich der korrekt Wohnort in der Datenbank befindet.\\
		Daraufhin werden mit diesen Eingaben Kombinationen für die Suche und die URL-Generierung erstellt. Mögliche Such-Kombinationen für erfolgreiche Ergebnisse sind:
		
		\textit{Vorname, Nachname, Wohnort/Standort;}\\
		\textit{Vorname, Nachname, Geburtsjahr;}\\
		\textit{Vorname, Nachname, Institution;}\\
		\textit{Vorname, Nachname, Wohnort/Standort, Geburtsjahr;}\\
		\textit{Vorname, Nachname, Wohnort/Standort, Institution;}\\
		\textit{Benutzername einer Social-Media-Seite;}
		
		
		Die Kombination aus vielen oder allen Daten ist ebenfalls eine mögliche Option. Allerdings wird dadurch oft kein Ergebnis gefunden, da nicht zu jeder Information ein Eintrag im Internet besteht.\\
		Sobald die Kombinationen aus den Daten bekannt sind, werden die Such-URLs für die Google-Suchmaschine generiert.
		\subsection{Generierung der Google-Such-URLs}
			\subsubsection{Aufbau eines URLs}
			\label{subsec:AufbauURL}
			Ein Uniform Resource Locator (URL) lokalisiert eine Ressource, indem eine abstrakte Identifikation der Lokalisierung verwendet wird. Dabei wird ein URL grundsätzlich im folgenden Format angegeben.\cite{RFC1738}
			
			$<scheme>:<scheme-specific-part>$ \cite{RFC1738}
			
			Das Schema gleicht hierbei meist dem verwendeten Protokoll wie HTTP oder FTP. Der Doppelpunkt stellt die Trennung zum Schema-spezifischen Teil dar. Ein Beispiel für ein HTTP-URL-Aufbau ist im Folgenden definiert.\cite{RFC1738}
			
			$http://<host>:<port>/<path>?<searchpart>$\cite{RFC1738}
			
			Hier wird das Protokoll HTTP als Schema verwendet, wobei sich der Aufbau bei der Verwendung des HTTPS-Protokolls kaum unterscheidet. Lediglich das Schema und der Port verändern sich.\\
			Für den <host> kann der Fully Qualified Domain Name (FQDN) oder die IP-Adresse des Hostrechners eingetragen werden. Wenn der Port nicht angegeben wird, ist der Standardport voreingestellt. Bei HTTP wäre dies Port 80 und bei HTTPS Port 443. Der <path> stellt ein HTTP-Selektor dar und ist mit einem Fragezeichen von der Suchzeichenkette getrennt.\cite{RFC1738}\\
			Im Bereich des <searchpart> lassen sich URL-Parameter einfügen um Informationen an die entsprechende Webseite mitzugeben. Die Parameter bestehen aus einem Schlüssel und aus einem Wert, welche durch ein Gleichheitszeichen getrennt werden. Um mehrere Parameter hinzuzufügen und zu kombinieren, wird das kaufmännische Und-Zeichen verwendet.\cite{GoogleURL}\\
			Ein URL für die Google-Suche von \textit{Max Mustermann} ist in dem folgenden Beispiel gegeben:
			
			$https://www.google.com/search?q=Max+Mustermann$
			
			Allerdings können URLs nur mit ASCII-Zeichen erzeugt und versendet werden. Aus diesem Grund müssen Zeichen, die nicht im ASCII vorkommen, in ein gültiges Format umgewandelt werden. Dies wird realisiert, indem die URL-Kodierung das nicht enthaltende ASCII-Zeichen durch ein "'\%"', gefolgt von zwei Hexadezimalen Ziffern, ersetzt. Beispielsweise repräsentiert "'\%20"' ein Leerzeichen und "'\%22"' ein Anführungszeichen. \cite{HTMLURL} \\
			
			\subsubsection{Erzeugung der Such-URLs}
			Dieser Absatz beschreibt die Erstellung der Such-URLs für Google mit dem Wissen aus Kapitel \ref{subsec:AufbauURL}.\\
			Für jede genannte Kombination aus den eingegebenen Daten werden Link-Muster erzeugt. Diese entsprechen einem Lückentext. Sobald die entsprechenden Muster ausgewählt wurden, werden die Lücken mit den Daten befüllt. Dadurch wird eine Liste mit einer variierenden Menge von Suchlinks erstellt. Diese Liste wird anschließend von dem Web Crawler verwendet, um die Suche zu starten.
			Ein URL für die Suche nach Information auf beliebigen Webseiten wird wie folgt dargestellt:
					
			\textit{https://www.google.com/search?q=\%22Max+Mustermann\%22+\%22Weingarten\%22}
			
			Wenn allerdings der Benutzername einer Social-Media-Seite bekannt ist, wird ein weiterer URL erstellt. Mit diesem wird speziell nach Einträgen auf der entsprechenden Webseite gesucht. Dazu kann der Operator "'site"' verwendet werden. Dieser beschränkt die Suchergebnisse soweit, dass die vorgeschlagenen Einträge ausschließlich auf einer festgelegten Webseite vorkommen. Das folgende Beispiel beschreibt die Suche nach dem Benutzer "'Mustermann"' auf der Webseite "'Instagram"'. Dabei ersetzt die ASCII-Zeichenkette "'\%3A"' den Doppelpunkt. \cite{HTMLURL}
			
			\textit{https://www.google.com/search?q=site\%3Ainstagram.com+\%22Mustermann\%22}
			
			\subsubsection{Optimierung der Such-URLs}
			\label{subsubsec:URLOptimieren}
			Um die Suchergebnisse von Google zu verbessern, können die Suchbegriffe in Anführungszeichen gesetzt werden. Dadurch wird eine Phrasensuche gestartet, die nach einer Zeichenfolge sucht. Das bedeutet, es wird ausschließlich nach diesen Zeichenfolgen gesucht und nicht nach einer Abwandlung. Ein Beispiel hierfür ist die Suche nach "'Michael Bazzell"'. Wenn diese Suche ohne Anführungszeichen durchgeführt wird, werden zusätzlich Webseiten vorgeschlagen die den Namen "'Mike Bazzell"' anstatt "'Michael Bazzell"' beinhalten. Diese erweiterte Suche kann dazu führen, dass unzählige Webseiten vorgeschlagen werden, die nicht unmittelbar etwas mit dem Thema der Suchbegriffe zu tun hat. Um dem vorzubeugen können Anführungszeichen verwendet werden, welche die Anzahl der Suchergebnisse um einen sehr großen Teil verringern. \cite{Bazzell}\\
			Für die Suche nach \textbf{Marco Lang} werden ungefähr \textbf{96.400.000} Ergebnisse mit Hilfe der Google-Suchmaschine gefunden. Wird die Suche mit den Anführungszeichen verfeinert indem nach \textbf{"'Marco"' "'Lang"'} gesucht wird, werden etwa \textbf{55.600.000} Ergebnisse gefunden. Allerdings werden hier Webseiten vorgeschlagen, welche die Wörter "'Marco"' und "'Lang"' beinhalten, jedoch müssen diese nicht direkt nebeneinander und auch nicht in der Reihenfolge vorkommen. Es wäre möglich, dass bei dieser Suche, Webseiten mit Verweisen auf die Namen "'Marco Mustermann"' und "'Max Lang"' vorgeschlagen werden. Aus diesem Grund kann nach \textbf{"'Marco Lang"'} gegoogelt werden. Dadurch wird die Anzahl der Suchergebnisse auf \textbf{45.500} Ergebnisse reduziert. Der Grund für die starke Reduzierung ist, dass ausschließlich die Webseiten vorgeschlagen werden, die den kompletten String "'Marco Lang"' beinhalten. Für eine weitere Optimierung der Ergebnisse wird der Wohnort hinzugefügt, wie in dem Beispiel \textbf{"'Marco Lang"' "'Tettnang"'}. Dadurch werden die Suchvorschläge auf lediglich \textbf{95} Ergebnisse reduziert. Der URL zu dieser optimierten Suche lautet: 
			
			\textit{https://www.google.com/search?q=\%22Marco+Lang\%22+\%22Tettnang\%22}
			
			Nicht nur die Reduzierung der Suchergebnisse, sondern auch das Herausfiltern von unerwünschten Webseiten hat einen positiven Effekt auf die zu erstellende Anwendung, da die vorgeschlagenen Seiten in den folgenden Schritten analysiert werden müssen. Das bedeutet, dass jede unerwünschte Seite, die allein durch die Suche herausgefiltert werden kann, einen großen Laufzeitvorteil mit sich bringt. 
			
			
		
		\subsection{Auswahl der Bibliothek für Serveranfragen}
			\subsubsection{Ziele und Anforderungen}
				Damit eine Person im Internet gesucht werden kann, muss das Programm in der Lage sein, Anfragen an einen Server zu versenden und die dazugehörigen Antwort zu empfangen. Dazu müssen Webseiten die JavaScript enthalten auslesbar sein.
			\subsubsection{Lösungsideen}
				Im Folgenden werden drei Möglichkeiten beschrieben, um Anfragen an einen Server zu versenden. Zum einen ist das die Python Request-Bibliothek, welche sich optimal für HTTP-Anfragen eignet.\cite{WebScraping} Zum anderen bietet sich die Verwendung eines automatisierten Webbrowsers an, was mit Hilfe der Selenium Python API realisierbar ist.\cite{lawson2015web} Über diese API ist es möglich, auf alle Funktionen des Selenium WebDrivers zuzugreifen.\cite{SeleniumWithPython} Eine Alternative dazu ist das Python Framework Scrapy, welches zum Crawlen von Webseiten und Extrahieren von Daten verwendet werden kann.\cite{Scrapy} Die letzte Möglichkeit stellt die  Scrapy Middleware Scrapy-Selenium dar.\cite{scrapy-selenium} Dadurch wird die Kommunikation von Scrapy und Selenium ermöglicht.
			\subsubsection{Bewertung der Lösungsideen anhand den Anforderungen}
				Für komplizierte Anfragen an einen Server eignet sich die Request-Bibliothek von Python sehr gut. Darüber hinaus ist der Umgang mit Cookies, Header, et cetera  einfach gestaltet. Auch die Generierung des Such-URLs wird von dieser Bibliothek übernommen. Des Weiteren hat Requests einen großen Laufzeit-Vorteil gegenüber dem automatisierten Webbrowser und kann HTTP-Fehlermeldungen empfangen. Allerdings lässt sich mit der Request-Bibliothek keine JavaScript-Seite auslesen.\\
				Wenn das Framework Scrapy standardmäßig verwendet wird, können ebenfalls keine JavaScript-Seiten ausgelesen werden. Doch in Scrapy lässt sich ein automatisierter Webbrowser einfügen, mit welchem das Auslesen von JavaScript-Webseiten möglich ist. Zusätzlich lässt sich mit Scrapy ein effektiver Web Crawler und Web Scraper entwickeln, was für die nächsten Schritte ein erheblicher Vorteil ist.\\
				Aus den erläuternden Gründen wird das Framework Scrapy mit der Verbindung eines automatisierten Webbrowsers für die Personensuche verwendet. Der automatisierte Webbrowser muss in dem Framework implementiert werden, da auf bestimmte Webseiten mit JavaScript direkt zugegriffen wird. Infolgedessen wird die Middelware Scrapy-Selenium verwendet, da sie die eine kompakte Möglichkeit bietet, den automatisierten Webbrowser in Scrapy zu implementieren. Durch diese Kombination aus Scrapy und dem Selenium WebDriver lassen sich JavaScript-Seiten problemlos auslesen. \\
				Zusätzlich zu diesem Framework wird ein unabhängiger Selenium-Wedriver implementiert. Dieser wird für den Umgang mit den Social-Media-Seiten benötigt, da auf diesen Seiten ein Login vollzogen werden muss. Das hat den Vorteil, dass die Anmeldung in der Session gespeichert wird. Somit muss bei einem erneuten Zugriff auf dieselbe Seite keine neue Anmeldung vollzogen werden.
	
		\subsection{Erstellung des Web Crawlers}
		Nachdem der Selenium WebDriver in das Scrapy Framework implementiert wurde, kann mit dem "'crawling"' begonnen werden. Der Web Crawler hat die Aufgabe, ausgewählte Social-Media-Seiten zu durchstöbern und den von Google vorgeschlagenen Webseiten zu folgen. Wie in Bild \ref{img:AufbauWebCrawler} gezeigt, werden zuerst die Informationen über die Zielperson eingelesen. Anschließend werden die Social-Media-Seiten behandelt. Dadurch können weitere Informationen über die Person gefunden werden. Die angegebenen Daten über die Zielperson werden zur Generierung der Google-Such-Links verwendet. Mit diesen Links und der Google-Suchmaschine werden Webseiten gesucht, die mögliche Inhalte betreffend der Zielperson enthalten. Im nächsten Schritt wird die Google-Webseite mit den Suchergebnissen analysiert und ausgelesen. Dadurch können die URLs für die entsprechenden Webseiten gewonnen werden. Diesen URLs wird anschließend gefolgt, um Informationen über die Zielperson zu gewinnen.\\
		
		\begin{figure}[h!]
			\centering
			\includegraphics[ scale=0.3]{bilder/webcrawler.png}
			\caption{Aufbau des entwickelnden Web Crawlers}
			\label{img:AufbauWebCrawler}
		\end{figure}
		
			\subsubsection{Umgang mit den Social-Media-Seiten}
			\label{subsubsec:SocialMediaSeiten}
			Zu den verwendeten Social-Media-Webseiten gehören Instagram, Facebook, Twitter, Xing und LinkedIn. Für diese Webseiten werden gefälschte Accounts und ein eigener Selenium WebDriver erstellt. Dieser automatisierte Webbrowser ist ausschließlich für die Behandlung von Social-Media-Seiten zuständig. Damit vollständige Profile angezeigt werden können, loggt sich dieser automatisch auf den entsprechenden Plattformen ein. Die gewonnen Informationen werden dem Profil der gesuchten Person hinzugefügt.\\
			Es besteht die Möglichkeit, dass nur der Benutzername einer Social-Media-Plattform über die Ziel Person bekannt ist. Weiter Attribute wie Vorname, Nachname und Wohnort sind demnach nicht angegeben. In diesem Fall kann mit einem einzigartigen Benutzername nach diesen Attributen auf der entsprechenden Webseite gesucht werden. Einen einzigartigen Benutzernamen wird von den Plattformen Instagram und Twitter verwendet. Aus diesem Grund kann die erweiterte Suche nur auf diesen beiden Plattformen umgesetzt werden.
			
			\paragraph{Login-Formulare}
			Der Selenium WebDriver muss sich nicht bei jeder Social-Media-Webseite einloggen. Zu Beginn wird kontrolliert, zu welcher Plattform ein Benutzername eingegeben wurde. Dazu gibt es bei dieser Anwendung die Möglichkeit einen Facebook-, Instagram- oder Twitter-Benutzername einzugeben. Je nach Eingabe meldet sich der Browser auf den entsprechenden Seiten an. Auf den Seiten Xing und LinkedIn wird sich standardmäßig  angemeldet, sobald der Vorname, Nachname und Wohnort eingegeben wurde.\\
			Bei der Umsetzung wird im ersten Schritt die Login-Seite der entsprechenden Plattform angefordert. Die Antwort wird mit Hilfe der BeautifulSoup-Bibliothek nach dem HTML-Tag <input> durchsucht. Allerdings hat nicht jede Webseite den komplett identischen Aufbau. Das bedeutet, dass sich bei diesen Tags die Attribute unterscheiden können. Infolgedessen unterscheidet sich die Auswahl der <input>-Tags auf den angeforderten Seiten.\\
			Die Anmeldung von Instagram, Facebook und LinkedIn sind nahezu identisch. Hier können die zwei gesuchten <input>-Felder mit Hilfe des Attributs "'type"' identifiziert und gefunden werden. Bei der Twitter-Login-Seite werden die Felder stattdessen mit Hilfe des Attributes "'class"' gesucht. Andernfalls findet der Browser keine interaktiven Elemente.\\
			Zum übertragen der Benutzernamen und Passwörter benötigt der Selenium WebDriver ein Element mit eindeutigem Attribut zur Referenzierung. Dafür dient bei Instragram, LinkedIn und Facebook das vorher gefundene Element mit dem Attribute "'id"'. Bei Twitter ist das dass Attribut "'class[0]"' und bei Xing "'name"'.
			 
			\paragraph{Instagram}
			Instagram und Twitter verwenden beide einen einzigartigen Benutzername. Dies ist ein bedeutender Vorteil für die Identifikation einer Person. Auf diesen Webseiten kann dadurch eine Person mit ausschließlich einem Benutzername identifiziert werden. Es ist ebenfalls möglich eine Person mit ihrem offiziellen Namen zu suchen und zu finden.\\
			Wenn der Instagram-Benutzername eingegeben wurde, wird die dazugehörige Profilseite angezeigt und nach Informationen durchsucht. Im nächsten Schritt dienen die vorgeschlagenen Freunde, welche in Beziehung zu diesem Profil stehen, als weitere Informationsquelle. Das bedeutet, dass diese Kontakte ebenfalls durchsucht werden. Bei einer Übereinstimmungen wird die Suche nach Kontaktinformationen abgebrochen. Die gefundene Person, sowie die übereinstimmende Daten, werden gespeichert und können später zur Generierung  der Phishing-Mail verwendet werden. Eine Übereinstimmung kann beispielsweise dieselbe Universität sein.\\
			Falls sich jedoch eine Instagram-Seite unter den Google-Vorschlägen befindet, und die Übereinstimmung des Profils mit der Zielperson nicht klar ist, können die vorgeschlagenen Kontakte für die Identifizierung der Zielperson genutzt werden. So wird beispielsweise erkannt, wenn ein Freund aus der selben Stadt vorgeschlagen wird, dass es sich um die gesuchte Person handeln kann. Diese Methode erzielt kein sicheres Ergebnis. Jedoch kann die Wahrscheinlichkeit erhöht werden, dass es sich um die richtige Person handelt.\\
			Eine Profilseite mit bekanntem Username  wird mit dem folgenden Link angefordert:
			
			\textit{https://www.instagram.com/username/}
			
			Weitere Seiten und Einträge der gesuchten Person, unabhängig von der Profilseite, können mit dem Suchbefehl \textbf{site:instagram.com "'username"' \\ -site:instagram.com/username} angezeigt werden. \cite{Bazzell} In der Anwendung wird dies mit dem folgenden URL umgesetzt.
					
			\textit{https://www.google.com/search?q=site\%3Ainstagram.com+\%22username\%22+-site\\
				\%3Ainstagram.com\%2Fusername\&oq=site\%3Ainstagram.com+\%22username\%22+-site\\
				\%3Ainstagram.com\%2Fusername}
			
			Die dazugehörigen Suchergebnisse werden anschließend gleich den normalen Google-Suchergebnissen behandelt.
			
			\paragraph{Twitter}
			Auf der Webseite Twitter wird ausschließlich die Profilseite nach Informationen durchsucht. Dies ist mit dem Link \textit{https://twitter.com/username} möglich.
			
			\paragraph{Facebook}
			Facebook bietet ein großes Potential um OSINT zu betreiben. Allerdings hat Facebook optimale Algorithmen zur Erkennung von automatisierten Crawlern entwickelt. Aus diesem Grund wurde das gefälschte Konto nach wenigen Versuchen gesperrt. Das hat zu Folge, dass auf dieser Plattform nur begrenzt gesucht werden kann, da keine Anmeldung vorgenommen wird. Um das Konto zu entsperren verlangt Facebook eine Kopie des Ausweises, ein Bild mit erkennbarem Gesicht und eine Handynummer.\\
			Aus diesen Gründen wird Facebook nur dann und ohne Anmeldung verwendet, wenn sich ein Vorschlag unter den Google-Suchergebnissen befindet.

			\paragraph{LinkedIn und XING}
			LinkedIn und XING bieten eine optimale Informationsquelle bezüglich der schulischen und beruflichen Tätigkeit der Zielperson. Allerdings gibt es hier keinen einzigartigen Benutzernamen. Demzufolge wird eine Person mit dem vollen Namen und dem aktuellen Wohnort gesucht. Dabei wird auf LinkedIn ein Filter angewendet, bei dem ausschließlich Personen aus Deutschland angezeigt werden. Ein Profil wird untersucht, wenn nur eine Person vorgeschlagen wird. Bei einer Mehrzahl von gefunden Personen werden diese nicht auf Informationen durchsucht, da keine Identifikation möglich ist.
			Die Personensuche bei LinkedIn, mit angwandtem Filter, wird mit dem URL
			
			\textit{https://www.linkedin.com/search/results/people/?facetGeoRegion=\%5B\%22de\%3A0\%22\%5\\D\&keywords=vorname\%20nachname\%20wohnort\&origin=FACETED\_SEARCH} 
			
			dargestellt. Bei Xing sieht der Such-URL wie folgt aus.
						
			\textit{https://www.xing.com/search/old/members?hdr=1\&keywords=vorname+nachname+\\wohnort}
			
			
			\subsubsection{Analyse der Google-Suchergebnisse}
			Zur Analyse der Webseite mit den Suchergebnissen von Google wird der Seitenquelltext benötigt. Mit Hilfe des Quelltextes, können  die entsprechenden Links erkannt werden. Der Seitenquelltext wird mit Hilfe der BeautifulSoup-Bibliothek angezeigt.\\
			Das Bild \ref{img:GoogleSuchergebnis} stellt ein Suchergebnis von Google dar. Der dazugehörige Quelltext befindet sich in der Darstellung \ref{lst:GoogleSeitenquelltext}.
			\begin{figure}[h!]
				\centering
				\includegraphics[ scale=0.65]{bilder/Google-Suchergebnis1.png}
				\caption{Container eines Google-Suchergebnisses \cite{suchergebnisseMarco}}
				\label{img:GoogleSuchergebnis}
			\end{figure}
			Im Ausschnitt des Seitenquelltextes \ref{lst:GoogleSeitenquelltext} ist zu sehen, dass der <div>-Container mit der Klasse "'g"' einen Hyperlink enthält, was an dem HTML-Tag <a> zu erkennen ist. Dieser Link wird für das Web Scraping benötigt. Deswegen wird genau nach diesem Link gesucht.\\
			Da der <div>-Container bei jedem Suchergebnis identisch ist, kann bei jedem Ergebnis nach dem entsprechenden Container gesucht werden. Anschließend kann der erste Link in diesem <div>-Tag ausgelesen werden. Dies wird mit Hilfe der BeautifulSoup-Bibliothek umgesetzt.\\ %TODO DIE oder DER URL
			Um zu erkennen, ob mehrere Seiten mit Suchergebnissen existieren, wird nach bestimmten Hyperlinks gesucht. Diese Links werden über das Attribute "'class"' identifiziert. Mit Hilfe der BeautifulSoup-Bibliothek wird nach dem Klassennamen "'fl"' gesucht. Falls weitere Seiten mit Suchergebnissen vorhanden sind, werden die dazugehörigen Links in einer Liste gespeichert. Diese Hyperlinks könnten dazu verwendet werden,  um weitere URLs zu finden. Allerdings wird in dieser Arbeit nur die erste Such-Ergebnisseite von Google als Quelle verwendet, damit die Suche analysierbar und bewertbar bleibt.\\
		
			\begin{lstlisting}[caption=Ausschnitt des Quelltextes von einem einem Google-Suchergebnis \cite{suchergebnisseMarco},label={lst:GoogleSeitenquelltext}]
			<div class="g">
				<h3 class="r">
					<a href="/url?q=https://www.fupa.net/spieler/marco-lang
						-1261543.html&amp;sa=U&amp;ved=0ahUKEwiZ3PDGqMvhAhWt
						URUIHU7VAcwQFggUMAA&amp;usg=AOvVaw2QiSMFzScB0Jcvo
						PCisBGw"><b>Marco Lang</b>- Spieler - FuPa - FuPa
					</a>
				</h3>	
			\end{lstlisting} 
			
\section{Die Personenidentifizierung}
	\subsection{Ziele und Anforderungen}
	Die zu entwickelnden Methoden haben das Ziel die Wahrscheinlichkeit, dass es sich um die gesuchte Person handelt, zu erhöhen. Dabei soll die Laufzeit und Wirksamkeit möglichst optimal sein. Allerdings dürfen hierfür nicht zu viele Informationsquellen ignoriert werden.
	
	\subsection{Lösungsideen}
	\label{sec:WannhandeltessichumdiegesuchtePerson}
	Bei jeder einzelnen Suche besteht die Herausforderung darin, zu erkennen, wann es sich um die gesuchte Person handelt. Durch die große Anzahl an verfügbaren Informationen im Internet besteht eine hohe Wahrscheinlichkeit, dass Personen mit sehr ähnlichen Profilen gefunden werden.\\
	Aus diesem Grund werden Maßnahmen getroffen um die gesuchte Person zu erkennen. Dafür ist der erste Schritt die Anzahl der Suchergebnisse zu reduzieren. Dies ist durch den Ansatz der Personensuche im Kapitel \ref{sec:Suche nach Information} möglich. Dabei wird abhängig von der eingegebenen Information die Suche variiert. Des Weiteren kann durch eine Optimierung des Such-URLs \ref{subsubsec:URLOptimieren} die Personensuche verfeinert und somit die Ergebnisse verbessert werden. Durch diese Maßnahmen steigt die Wahrscheinlichkeit, dass es sich um die richtige Person handelt.\\
	Im zweiten Schritt können die folgenden Methoden angewendet werden.

		\subsubsection{Generierung eines Identifikationsschlüssel}
		\label{subsubsec:VorlaeufigInhaltskontrolle}
		Bei der Personensuche wird mit Hilfe der eingegebenen Daten nach einer Person gesucht. Dabei können fehlerhafte Webseiten von Google vorgeschlagen werden. Fehlerhaft bedeutet hier, dass die Webseiten einen Inhalt repräsentieren, welcher nicht mit der gesuchten Person übereinstimmt.\\ 
		Um dem entgegenzuwirken, können bekannte Informationen als Identifikationsschlüssel verwendet werden. Allerdings müssen diese einzigartige Daten sein. Dazu zählt beispielsweise die E-Mail-Adresse oder Benutzernamen von den Plattformen Instagram und Twitter. Der vollständige Name ist nicht einzigartig und dient deswegen nicht als Identifikationsschlüssel. Das bedeutet, dass es mehrere Personen mit demselben vollständigen Namen geben kann.\\
		Um eine Person zu identifizieren, zu welcher keine einzigartigen Informationen bekannt sind, können Kombinationen aus den angegebenen Daten erstellt werden. Diese Kombinationen dienen in diesem Fall als Identifikationsschlüssel. Im folgenden sind alle möglichen Kombinationen aufgelistet.
		
		\textit{Vorname, Nachname, Wohnort/Standort;}\\
		\textit{Vorname, Nachname, Geburtsjahr;}\\
		\textit{Vorname, Nachname, Institution;}
		%\textit{Vorname, Nachname, Facebook-Benutzername;} %TODO in Projekt realisieren	
		
		Der Webseitentext kann anschließend auf das Vorkommen des Identifikationsschlüssels kontrolliert werden. Wenn der Text nur eine dieser Kombination beinhaltet, wird diese Seite für die Informationsgewinnung verwendet. Andernfalls wird die Webseite verworfen.\\

		\subsubsection{Kontaktanalyse}	
		In den Suchergebnissen kann eine Profilseite von einer Social-Media-Plattformen vorgeschlagen werden. Dabei ist nicht bekannt, ob dieser Benutzer mit der gesuchten Person übereinstimmt. Aus diesem Grund besteht die Möglichkeit einer veränderten Form der Kontaktanalyse, wie in Kapitel \ref{subsec:Kontaktanalyse}, beschrieben wurde. Hierbei werden die Kontakte ebenfalls durchsucht. Bei einer Übereinstimmung von Daten des Kontaktes mit Attributen der gesuchten Person wird angenommen, dass es sich bei dem gefundenem Profil um die korrekte Person handelt. Die Annahme wird getroffen, da Kontakte bzw. Freunde dadurch eine Beziehung zum Opfer aufweisen. Ein Beispiel hierfür wäre der selbe Standort.\\
		Bei keiner Übereinstimmung von Informationen wird die vorgeschlagene Webseite verworfen.	

	\subsection{Bewertung der Lösungsideen anhand den Anforderungen}
	Beide Methoden zur Identifizierung einer Person können eine Verbesserungen der Ergebnisse mit sich bringen. Die Wahrscheinlichkeit wird erhöht, dass es sich um die korrekte Person handelt. \\
	Die Methoden unterscheiden sich in der Wirksamkeit und in der Laufzeit. Durch die Verwendung beider Methoden wird die Anzahl von Fehlinformationen in dem Profil der gesuchten Person reduziert. Allerdings können gleichzeitig wichtige Informationsquellen ignoriert werden, wenn diese den Kriterien nicht entsprechen.\\
	Das Ergebnis der Kontaktanalyse ist nicht optimal. Es kann nicht davon ausgegangen werden, dass es sich bei einer Übereinstimmung von einem  Attribut unmittelbar um die Zielperson handelt. Beim Betrachten der Laufzeit,  kann davon ausgegangen werden, dass die Kontaktanalyse deutlich mehr Zeit und Ressourcen benötigt.\\
	Es wird die Methode zur Verwendung eines Identifikationsschlüssels umgesetzt. Dadurch werden ausschließlich die Webseiten behandelt, welche den Schlüssel zur Identifizierung beinhalten. Das bedeutet, dass nur identifizierbare Profile durchsucht werden. Das hat zur Folge, dass manche Profile nicht beachtet werden. Dennoch werden dadurch weitestgehend fehlerhafte Informationen in dem Opferprofil vermieden.


\section{Umsetzung der Methode zur Verwendung eines Identifikationsschlüssels}
Zu Beginn der vorläufigen Inhaltskontrolle werden die Eingaben abgefragt. Dadurch wird erkannt, zu welche Daten Informationen vom Benutzer eingegeben wurden. Anschließend werden mit diesen Daten alle möglichen Kombinationen aus Kapitel \ref{subsubsec:VorlaeufigInhaltskontrolle} erstellt. Es sind allerdings nur die Kombinationen möglich, für die die Daten bekannt sind.\\
Für die Suche des Vornamen und Nachnamen wird ein String erzeugt, der beide Attribute kleingeschrieben beinhaltet. Ein korrekter String ist "'max mustermann"'. Infolgedessen wird der Webseitentext zu einem String umgewandelt. Anschließend wird kontrolliert, ob sich der String bestehend aus Vornamen, Nachnamen und beispielsweise Wohnort in dem Webseitentext befindet. Wenn diese Abfrage korrekt ist, wird die Webseite weiter behandelt und es kann nach Information gesucht werden.

\section{Herausfiltern von wichtigen Informationen auf einer Webseite}
	\subsection{Ziele und Anforderung}
	Bei der Suche nach einer ausgewählten Person können verschiedenste Arten von Webseiten gefunden werden. Aus diesem Grund muss das Programm eine gewisse Intelligenz mit sich bringen, um die wichtigsten Daten aus einer Seite herauszufiltern. Dabei ist es nicht möglich, festgelegte Bereiche einer Webseite durch eine Hartkodierung auszulesen, da jede Webseite eine individuelle Struktur hat.
	\subsection{Lösungsideen}
	\label{subsec:ErkennenVonInformation}	
	Zu Beginn werden Wortsammlungen erstellt. Diese Wortsammlungen sind Listen, welche aussagekräftige Schlüsselwörter enthalten und nach Themen kategorisiert sind. Beispiele für den Inhalt dieser Listen sind alle Hochschulen- und Universitätsnamen in Deutschland, Berufsbezeichnungen und Tätigkeiten, Hobbybezeichnungen sowie alle Städte und Gemeinden in Deutschland. Die Wortsammlungen werden mit Hilfe von bekannten Listen im Internet eigenständig befüllt. Als Informationsquelle dienen alle öffentlich frei zugänglichen Quellen, die hilfreiche Informationen enthalten. Die erzeugten Listen werden in den kommenden Schritten verwendet, um wichtige Informationen herausfiltern zu können.
	
	Die Grundidee zum Erkennen von wichtigen Informationen ist die Analyse des vorliegenden Webseiten-Textes. Eine Methode zur Textanalyse ist die automatisierte Schlüsselwort-Gewinnung. Hierbei wird die HTML-Seite zu einem verwendbaren Text formatiert, wobei alle Sonderzeichen herausgefiltert werden. Im nächsten Schritt werden Schlüsselwörter aus dem formatierten Webseitentext generiert und in einer Liste gespeichert. Diese Liste kann darauf mit den erzeugten Wortsammlungen verglichen werden. Bei einer Übereinstimmung eines Schlüsselwortes wird das Wort mit der entsprechenden Kategorie vorgemerkt und später in die verwendete Speicherstruktur eingetragen.\\
	Eine weitere Methode zur Textanalyse ist der Vergleich von Zeichenketten. Dabei wird der vorliegende Webseitentext in einen String umgewandelt. Anschließend kann kontrolliert werden, ob Elemente aus den Wortsammlungen in diesem Text vorkommen. Auch hier kann bei einer Übereinstimmung das Wort mit entsprechenden Kategorie gespeichert werden.
	
	\subsection{Bewertung der Lösungsideen anhand den Anforderungen}
	Für die Methode, bei der ausschließlich Zeichenketten verglichen werden, besteht die Möglichkeit, alle Elemente der Wortsammlung zu erkennen. Dabei ist nicht relevant, aus wie vielen Wörtern und Zeichen ein Element besteht. Allerdings kann diese Methode zu falschen Ergebnissen führen. Zum Beispiel bei der Suche nach dem Wort "'test"'. Hierbei wird eine Übereinstimmung mit dem Wort "'testament"' gefunden, da die Zeichenfolge des gesuchte Elements in diesem Wort vorkommt. Trotzdem haben beide Elemente eine unterschiedliche Bedeutung. \\
	Ein Nachteil der Schlüsselwortgenerierung ist die festgelegte Anzahl an Wörter, aus dem ein Schlüsselwort besteht. Damit Elemente mit mehreren Wörtern gefunden werden können, müssten N-Gramme erstellt und den Schlüsselwörtern hinzugefügt werden.  Andernfalls werden ausschließlich Wörter, bestehend aus einem Wort, in Betracht gezogen. Im Gegensatz zur vorherigen Methode, kann mit dem Verfahren zur Schlüsselwortgenerierung die Anzahl des Vorkommens eines Wortes gezählt werden. Diese Anzahl ist wichtig für die Auswahl der Information. Darüber hinaus entstehen hierbei keine Fehler durch falsche Interpretationen.\\
	Aus den erwähnten Gründen wird für das Herausfiltern von wichtigen Informationen die Methode der Schlüsselwortgenerierung verwendet.


\section{Umsetzung der Methoden zum Herausfiltern von wichtigen Informationen}
	\subsection{Text formatieren}
	\label{subsec:TextFormatieren}
	Bevor die Schlüsselwörter generiert werden können, muss der Text in ein verwertbares Format umgewandelt werden. Aus diesem Grund wird der Seitenquelltext zuerst mit Hilfe des Python-Skripts html2text zu einem ASCII Plaintext umgewandelt.\cite{html2text} Anschließend werden Zeilenumbrüche und Sonderzeichen aus diesem Text herausgefiltert. Einzelne Wörter und Zahlen, die weniger als zwei Zeichen beinhalten, werden ebenfalls aussortiert. Nachdem der Text in ein verwertbares Format umgewandelt wurde, kann mit der Umsetzung für die automatisierte Schlüsselwortgenerierung mit NLP begonnen werden.
	
	\subsection{Erstellung der Wortsammlungen}
	Die Sammlung mit Schlüsselworten soll manuell erstellbar und beliebig erweiterbar sein. Die Anwendung muss ohne aufwendige Zugriffe von diesen Listen lesen können. Als mögliches Speichervariante dient eine CSV-Datei oder eine SQL-Datenbank.\\
	Die SQL-Datenbank ist ein komplexeres System. Infolgedessen werden aufwendige Zugriffe benötigt. Die CSV-Datei bringt alle Anforderungen mit sich. Es ist unkompliziert, diese manuell zu befüllen und beliebig zu erweitern. Darüber hinaus kann eine CSV-Datei ohne großen Aufwand mit Hilfe eines Python-Skriptes ausgelesen werden. Die erwähnten Gründe sprechen für die Verwendung einer CSV-Datei.
	
	Die Informationsgewinnung ist abhängig von diesen Wortsammlungen. Das bedeutet, dass nur die Informationen herausgefunden werden können, welche als Element in einer Wortsammlung vorkommt. Infolgedessen hat die Umsetzung der Wortsammlungen einen hohen Stellenwert.
	
		
		\subsubsection{Umsetzung der Wortsammlungen}	
		Für eine sinnvolle Informationsgewinnung werden die Wortsammlungen kategorisiert. Dabei entsprechen die Kategorien einem Teil der zu suchenden Personenattributen, wie in Bild \ref{img:personClass} dargestellt wird. Demzufolge gibt es die Kategorie "'Tätigkeiten"', "'Hobbys"', "'Institutionen"' sowie "'Städte und Gemeinden"'. Dabei enthalten die Wortsammlungen möglichst alle Bezeichnungen und Namen dieser Kategorien. 
		In Wortsammlung für Institution sind sowohl Firmennamen als auch Universitäts- und Hochschulnamen aufgelistet. Befüllt werden die Sammlungen aus öffentlich zugänglichen Listen im Internet. Die Tabelle \ref{img:wortsammlungQuellen} zeigt die Kategorien der Wortsammlungen und die dazugehörigen Informationsquellen. Zur Veranschaulichung ist ein Ausschnitt der Liste mit den Tätigkeiten im Anhang \ref{cha:anhang1} beigefügt. \\
		
		\begin{table}[h!]
			\centering
			\begin{tabular}{ | c | c | c | c|}
				\hline
				\textbf{Kategorie}		& \multicolumn{3}{p{11cm}|}{ \centering\textbf{Informationsquelle} }\\ \hline
				\hline
				Tätigkeiten 			& \multicolumn{3}{p{10cm}|}{\raggedright http://planet-beruf.de/schuelerinnen/mein-beruf/berufe-von-a-z} \\ \hline
				Hobbys 					& \multicolumn{3}{p{10cm}|}{\raggedright https://hobbyfuchs.org/hobbyliste-a-z;\\ https://hobbeasy.de/hobbys-finden-liste;\\https://www.langeweile-tipps.de/hobby-finden-die-ultimative-liste-der-hobbys} \\ \hline
				Institutionen			& \multicolumn{3}{p{10cm}|}{\raggedright https://de.wikipedia.org/wiki/Liste\_der\_Hochschulen\_\\in\_Deutschland;\\https://www.firmenfinden.de (Firmennamen nur von Kreis Bodensee und Ravensburg)} \\ \hline
				Städte und Gemeinden	& \multicolumn{3}{p{10cm}|}{\raggedright https://www.deutschland-auf-einen-blick.de/index.html} \\ \hline
			\end{tabular}
			\label{img:wortsammlungQuellen}
			\caption{Informationsquellen der Wortsammlungen}
		\end{table}
		\FloatBarrier
	\subsection{Automatisierte Schlüsselwortgenerierung}
		\subsubsection{Ziele und Anforderungen}
		Aus einem vorliegenden Webseitentext sollen Schlüsselwörter generiert werden. Dafür muss jedes Wort in diesem Text betrachtet und als Schlüsselwort erkannt werden. Des Weiteren soll die Möglichkeit bestehen, dass die Anzahl, aus wie vielen Wörtern ein Schlüsselwort besteht, festgelegt werden kann.
		 
		\subsubsection{Lösungsideen}
		Möglichkeiten zur automatisierten Schlüsselwortgenerierung stellen die Verfahren Rapid Automatic Keyword Extraction (RAKE) und Automatic Keyword Extraction mit NLP dar.
		
		\paragraph{RAKE}
		RAKE ist eine sehr effiziente Methode zur Schlüsselwortgenerierung. Die Funktion von RAKE basiert darin, dass Schlüsselwörter mehrere Wörter mit inhaltlicher Relevanz enthalten, allerdings selten Stoppwörter und Sonderzeichen.\cite{rose2010automatic}\\
		Als Stoppwörter werden Wörter bezeichnet, die sehr oft auftreten und keinen großen Informationsgewinn mit sich bringen. Beispiele dafür sind \textit{und}, \textit{weil}, \textit{der} oder \textit{als}.\cite{Stopwords}
			\begin{figure}[h!]
				\fbox{\parbox{\linewidth}{In einer jungen Wissenschaft wie der Informatik mit ihrer Vielschichtigkeit und ihrer unüberschaubaren Anwendungsvielfalt ist man oftmals noch bestrebt, eine Charakterisierung des Wesens dieser Wissenschaft und Gemeinsamkeiten und Abgrenzungen zu anderen Wissenschaften zu finden. Etablierte Wissenschaften haben es da leichter, sei es, dass sie es aufgegeben haben, sich zu definieren, oder sei es, dass ihre Struktur und ihre Inhalte allgemein bekannt sind.}}
				\caption{Beispieltext \cite{schubert2011didaktik}}
				\label{fig:text}
			\end{figure}\\
			Zu Beginn wird der zu analysierende Text, hier der Beispieltext in Bild \ref{fig:text}, durch einen Worttrenner aufgeteilt. Die entstehenden Fragmente, welche als mögliche Schlüsselwörter dienen, werden in ein Array gespeichert. Das erzeugte Array wird anschließend in Sequenzen von zusammenhängenden Wörtern unterteilt. Dabei erhalten die Wörter in einer Sequenz die gleiche Position und Reihenfolge wie im Ursprungstext und dienen gemeinsam als Kandidatenschlüsselwort.\cite{rose2010automatic}\\	
			Nachdem die möglichen Schlüsselwörter identifiziert sind, wird für jeden einzelnen Kandidaten ein Score berechnet. Dieser besteht aus dem Quotient des Grades $deg(w)$ und der Häufigkeit des Vorkommens eines Wortes innerhalb der Kandidaten $freq(w)$. Daraus ergibt sich die Formel:
			\begin{center}
				$deg(w)/freq(w) $
			\end{center}	
			
			Dabei beschreibt der Grad eines Wortes das gemeinsame Auftreten mit sich selbst und anderen Schlüsselwörtern. In der Tabelle \ref{tab:Co-occurance} ist der Grad für jedes Wort ablesbar, indem die Einträge in der entsprechenden Reihe summiert werden. Beispielsweise beträgt der Grad des Wortes \textit{"'Wissenschaft"'} den Wert \textit{3}. Dies ergibt sich aus der Rechnung:
			\begin{center}
				$2 + 1 = 3$
			\end{center}
			Das Wort \textit{"'Wissenschaft"'} kommt hier selbst zweimal in dem Kandidaten-Array vor und davon einmal in Verbindung mit dem Worten "'jungen"'.\\
			Die Häufigkeit des Vorkommens eines Wortes lässt sich ebenfalls in der Tabelle \ref{tab:Co-occurance} ablesen. Allerdings muss hier die Reihe und Spalte des jeweiligen Wortes abgeglichen werden. Für das Wort \textit{"'Wissenschaft"'} beträgt die Häufigkeit des Vorkommens den Wert \textit{3}.\\
			Zusammenfassend kann gesagt werden, dass \textit{deg(w)} die Kandidaten bevorzugt, welche oft und in langen Schlüsselwörtern, die mehrere Wörter enthalten, vorkommen. Dies bedeutet, dass beispielsweise \textit{deg(etabliert)} eine höhere Bewertung als \textit{deg(informatik)} bekommt, obwohl beide Wörter gleich oft im Text vorkommen. Dagegen wird bei \textit{freq(w)}, ausschließlich die Häufigkeit des Vorkommens bewertet. Bei der Formel \textit{deg(w)/freq(w)} werden die Wörter bevorzugt, welche überwiegend in langen Kandidatenwörtern vorkommen. Diese Formel bietet dadurch einen guten Mittelweg zur Schlüsselwortgewinnung. Ein Beispiel dafür sind die Wörter \textit{"'Wissenschaften} und \textit{"'allgemein"'}. Hier ist der Quotient von \textit{deg(allgemein)/freq(allgemein)} höher als von \textit{deg(Wissenschaften)/freq(Wissenschaften)}, obwohl die Häufigkeit des Wortes \textit{"'Wissenschaften"'} höher und der Grad gleich hoch ist. \cite{rose2010automatic}\\
			Durch das genannte Verfahren und der Formel \textit{deg(w)/freq(w)} für die Bewertung, ergeben sich die im Bild \ref{fig:SchlüsselwörterMitScore} dargestellten Kandidaten mit den dazugehörigem Endbewertungen. \cite{rose2010automatic}
			
			\begin{center}
				\begin{table}[h!]
					\scriptsize
					\begin{tabular}{*{24}{l|}}				
						\rotatebox[origin=c]{90}{} 
						&\rotatebox[origin=c]{90}{wissenschaften} &\rotatebox[origin=c]{90}{wissenschaft} &\rotatebox[origin=c]{90}{sei} &\rotatebox[origin=c]{90}{etablierte} &\rotatebox[origin=c]{90}{informatik} &\rotatebox[origin=c]{90}{aufgegeben} &\rotatebox[origin=c]{90}{gemeinsamkeiten} &\rotatebox[origin=c]{90}{oftmals} &\rotatebox[origin=c]{90}{charakterisierung} &\rotatebox[origin=c]{90}{jungen} &\rotatebox[origin=c]{90}{inhalte} &\rotatebox[origin=c]{90}{allgemein} &\rotatebox[origin=c]{90}{bekannt} &\rotatebox[origin=c]{90}{struktur} &\rotatebox[origin=c]{90}{wesens} &\rotatebox[origin=c]{90}{bestrebt} &\rotatebox[origin=c]{90}{unüberschaubaren} &\rotatebox[origin=c]{90}{anwendungsvielfalt} &\rotatebox[origin=c]{90}{definieren} &\rotatebox[origin=c]{90}{abgrenzungen}
						&\rotatebox[origin=c]{90}{leichter}
						&\rotatebox[origin=c]{90}{finden}
						&\rotatebox[origin=c]{90}{vielschichtigkeit}\\
						\hline
						wissenschaften & 2 & & & 1 & & & & & & & & & & & & & & & & & & &\\
						\hline
						wissenschaft & & 2 & & & & & & & & 1 & & & & & & & & & & & & & \\
						\hline
						sei & & & 1 & & & & & & & & & & & & & & & & & & & &	\\
						\hline
						etablierte & 1 & & &1 & & & & & & & & & & & & & & & & & & &	\\
						\hline
						informatik & & & & &1 & & & & & & & & & & & & & & & & & & \\
						\hline
						aufgegeben & & & & & &1 & & & & & & & & & & & & & & & & &	\\
						\hline
						gemeinsamkeiten & & & & & & & 1& & & & & & & & & & & & & & & &	\\
						\hline
						oftmals & & & & & & & & 1& & & & & & & & & & & & & & &\\
						\hline
						charakterisierung & & & & & & & & & 1& & & & & & & & & & & & & & \\
						\hline
						jungen & & 1 & & & & & & & & 1 & & & & & & & & & & & & &	\\
						\hline
						inhalte & & & & & & & & & & & 1 & 1 & 1 & & & & & & & & & &	\\
						\hline
						allgemein & & & & & & & & & & & 1 & 1 & 1 & & & & & & & & & & \\
						\hline
						bekannt & & & & & & & & & & & 1 & 1 & 1 & & & & & & & & & &	\\
						\hline
						struktur & & & & & & & & & & & & & &1 & & & & & & & & &	\\
						\hline
						wesens & & & & & & & & & & & & & & &1 & & & & & & & &\\
						\hline
						bestrebt & & & & & & & & & & & & & & & & 1& & & & & & & \\
						\hline
						unüberschaubaren & & & & & & & & & & & & & & & & & 1 & 1 & & & & &	\\
						\hline
						anwendungsvielfalt & & & & & & & & & & & & & & & & & 1 & 1 & & & & &	\\
						\hline
						definieren & & & & & & & & & & & & & & & & & & & 1 & & & & \\
						\hline
						abgrenzungen & & & & & & & & & & & & & & & & & & & & 1 & & &	\\
						\hline
						leichter & & & & & & & & & & & & & & & & & & & & & 1 & &	\\
						\hline
						finden & & & & & & & & & & & & & & & & & & & & & & 1 & \\
						\hline
						vielschichtigkeit & & & & & & & & & & & & & & & & & & & & & & & 1	\\
						\hline
					\end{tabular}
					\label{tab:Co-occurance}
					\caption{Co-occurance}
				\end{table}
			\end{center}
			
			\begin{figure}[h!]
				\fbox{\parbox{\linewidth}{ inhalte allgemein bekannt (9.0), unüberschaubaren anwendungsvielfalt (4.0), jungen wissenschaft(3.5), etablierte wissenschaften (3.5), wissenschaften (1.5), wissenschaft (1.5), wesens (1.0), vielschichtigkeit (1.0), struktur (1.0), sei (1.0), oftmals (1.0), leichter (1.0), informatik (1.0), gemeinsamkeiten (1.0), finden (1.0), definieren (1.0), dass (1.0), charakterisierung (1.0), bestrebt (1.0), aufgegeben (1.0), abgrenzungen (1.0)}}
				\caption{Schlüsselwörter mit zugehörigem Score}
				\label{fig:SchlüsselwörterMitScore}
			\end{figure}
			\FloatBarrier
			
			\paragraph{Automatic Keyword Extraction mit NLP}
			Bei dieser Methode wird ein vorliegende Text in die einzelnen Wörter unterteilt. Dabei wird eine Liste mit potentiellen Schlüsselwörtern erstellt, in der \textit{Stoppwörter} und Sonderzeichen herausgefiltert werden. Bei den Schlüsselwörtern handelt es sich nicht ausschließlich um ein Wort sondern auch um Wortsequenzen. Sogenannte N-Gramme bestehen aus einer festgelegten Anzahl von Wörtern. Dies hat den Vorteil, dass nicht nur Schlüsselwörter bestehend aus einem Wort erstellt werden können, sondern auch Schlüsselwörter mit Fragmenten eines Textes. Diese Art von Schlüsselwort wird benötigt um Informationen wie "'Bad Waldsee"' herauszulesen.\\
			Erweiternd kann die Anzahl der Schlüsselwörter mit dem Verfahren von Stemming reduziert werden. Die Verwendung von ergänzende Regeln, wie eine Mindestanzahl von Buchstaben in einem Wort, können die Schlüsselwörter weiter begrenzen.
	
		\subsubsection{Bewertung der Lösungsideen anhand den Anforderungen}
		RAKE stellt eine fertige Methode dar, um Schlüsselwörter, die den Inhalt eines Textes in kurz wiedergeben, zu erstellen. Dabei hat ein Anwender kaum Möglichkeiten eigene Implementierungen vorzunehmen, da vieles vorgegeben ist. In der zu erstellenden Anwendung soll jedoch nicht der Inhalt eines Textes in Schlüsselwörter zusammengefasst werden, sondern es wird nach informationsreichen Wörtern gesucht. Aus diesem Grund ist jedes einzelne Wort aus dem Webseitentext von Bedeutung. Dies spricht gegen RAKE, da es nur die selbst errechnenden Favoriten-Schlüsselwörter zur Verfügung stellt. Dadurch werden viele Wörter nicht in Betracht gezogen oder für weiterführende Bearbeitungen nicht bereitgestellt.\\
		Die Methode zur automatisierten Schlüsselwortgenerierung mit NLP bringt dagegen eine eigene Implementationsmöglichkeit mit sich. Das bedeutet, es kann selbst festgelegt werden, aus wie vielen Wörtern die Schlüsselwörter bestehen sollen. Des Weiteren wird jedes einzelne Wort in Betracht gezogen und verwendet.\\ Die Suche nach einer E-Mail-Adresse im Text lässt sich bei beiden Methoden hinzufügen. Jedoch wird aus den eben genannten Vorteilen die Information mit Hilfe der Methode zur automatisierten Schlüsselwortgewinnung mit NLP herausgefiltert.
		
	\subsection{Umsetzung des Automatic Keyword Extraction mit NLP}
		\label{subsec:AutomaticKeywordExtractionNLP}
		Durch das Natural Language Toolkit (NLTK) von Pyhton ist es möglich, den vorhandenen Webseitentext zu analysieren.\\
		Zu Beginn wird der vorhandene Text in einzelne Wörter zerlegt und in eine Liste gespeichert. Aus dieser Liste werden die "'stopwords"' der deutschen als auch der englischen Sprache herausgefiltert. Dadurch verringert sich die Anzahl der zu suchenden Wörter. Anschließend werden Bigramme aus dem zerlegten Text erstellt und der Liste mit den einzelnen Wörtern hinzugefügt. Dadurch können Elemente bestehend aus einem Wort sowie aus zwei Wörtern gefunden werden.\\
		Im nächsten Schritt wird die Liste mit den entsprechenden Wortsammlungen verglichen. Dabei werden die übereinstimmenden Wörter dem korrekten Personenattribut hinzugefügt.\\
		Die Wortsammlung der Institutionen wird nicht mit der Schlüsselwortliste verglichen. Die Problematik besteht darin, dass die Institutionsnamen aus verschieden vielen Wörtern bestehen können. Für die Erkennung solcher Namen müsste ein große Anzahl von N-Grammen erstellt und der Schlüsselwortliste hinzugefügt werden. Das würde allerdings zu einem großen Laufzeitnachteil führen. Demnach wird der Institutionsname in dem unformatierten Webseitentext gesucht. Der Text wird lediglich auf das erscheinen der korrekten Zeichenkette kontrolliert. Dadurch kann das Vorkommen der Institution auf einer Webseite nicht gezählt werden. Es kann nur die Anzahl der Webseiten, welche diese Zeichenkette beinhalten, festgestellt werden. Dennoch ist die Fehlerrate gering, da die langen Zeichenketten oft einzigartig sind.
		

	\subsection{Herausfiltern von Geburtsjahren}
		Das Geburtsjahr ist für die Generierung der E-Mail wichtig. Viele Personen verwenden eine Kombination aus dem bürgerlichen Namen und dem Geburtsjahr als lokalen Teil der E-Mail-Adresse. Aus diesem Grund wird speziell nach dem Geburtsjahr in den generierten Schlüsselwörtern aus Kapitel \ref{subsec:AutomaticKeywordExtractionNLP} gesucht.\\
		Dazu wird eine Suche nach einer vierstelligen Zahl, welche größer als 1900 und kleiner-gleich 2019 ist, durchgeführt. Beim Fund einer Zahl werden  fünfzehn Schlüsselwörter vor und hinter der vermutlichen Jahreszahl kontrolliert. Die Zahl wurde dabei auf fünfzehn festgelegt, da die Mehrzahl der geschriebenen Sätze eine Anzahl von 12-15 Wörtern enthält. \cite{seibicke1969schreibt} Falls dabei das Wort "'Geburtsdatum"', "'Alter"', "'geboren"', "'Geburtsort"', "'born"', oder "'birth"' vorkommt, wird das entsprechende Jahr als Geburtsjahr der Zielperson festgelegt. Die Such nach den Signalwörtern wird mit Hilfe des folgenden regulären Ausdrucks durchgeführt:
		\textbf{r"'((geburtsdatum)|(alter)|(geboren)|(geburtsort)|(born)|(birth))"'}.
		
		 Das Bild \ref{img:AlgoSucheNachGeburtsjahr} zeigt die Funktion des Algorithmus zur Suche nach dem Geburtsjahr. Dabei wurde das Jahr "'1995"' gefunden. Somit werden 15 Wörter vor und nach dieser Jahreszahl kontrolliert. Falls der Algorithmus vorzeitig eine Übereinstimmung mit dem regulären Ausdruck findet, wird die Suche, wie in diesem Beispiel, abgebrochen. Der rote Pfad im Bild \ref{img:AlgoSucheNachGeburtsjahr} zeigt auf das korrekt Signalwort "'geboren"'. Dadurch wird die Suche nach dem fünften Schritt erfolgreich beendet und die Jahreszahl "'1995"' dem Opferprofil als Geburtsjahr hinzugefügt.

		\begin{figure}[h!]
			\centering
			\includegraphics[ scale=0.25]{bilder/get_year_of_birth.png}
			\caption{Funktion des Algorithmus zur Suche nach dem Geburtsjahr}
			\label{img:AlgoSucheNachGeburtsjahr}
		\end{figure}
	
	\subsection{Herausfiltern von E-Mail-Adressen}
		Zu Beginn wird der unformatierte Webseitentext in Textfragmente zerlegt. Das Leerzeichen dient dabei als Trennelement. Anschließend werden die erzeugten Fragmente mit dem regulären Ausdruck, \textbf{r"'(.*((@)|(\(at\))).*\.(de|com|net)).*)"'}, nach einer gültigen E-Mail-Adresse durchsucht. Bei einer Übereinstimmung des regulären Ausdrucks wird der korrekte Teilstring und somit die E-Mail-Adresse ausgelesen. 
		
		\subsubsection{Auswahl einer E-Mail-Adresse}
		Es werden nur die E-Mail-Adressen herausgesucht, welche einen Bezug zur Zielperson haben. Aus diesem Grund wird der lokale Teil aller gefundenen Adressen mit dem Vor-und Nachnamen der Zielperson verglichen. Mit Hilfe der "'difflib"' und dem implementierten "'SequenceMatcher"' von Python lassen sich diese beide Sequenzen vergleichen und es wird ein prozentuale Übereinstimmung berechnet. Zur Differenzierung, ob eine E-Mail-Adresse eine Verbindung zum Opfer hat oder nicht, wird eine Prozent-Grenze bestimmt. Diese Grenze wurde aus den Ergebnissen von zahlreichen selbst durchgeführten Tests auf die Zahl 0,5 \% festgelegt. Dafür wurden Wertungen für verschiedene E-Mail-Adressen mit dem dazugehörigen Namen berechnet. Das folgende Beispiel soll die Methode zur Erkennung von korrekten E-Mail-Adressen verdeutlichen.\\
		In diesem Beispiel heißt die Person "'Max Mustermann"' und es werden zwei E-Mail-Adresse gefunden. Die erste Adresse lautet \textit{MusterMax@gmail.com} und die zweite \textit{MartaFrau@gmx.de}. Im ersten Schritt wird der Name "'Max Mustermann"' zu einem String "'maxmustermann"' umgewandelt. Im nächsten Schritt werden die lokalen Namen aus den E-Mail-Adressen herausgelesen und gleichzeitig in Kleinbuchstaben umgewandelt. In diesem Fall wäre das "'mustermax"' und "'martafrau"'. Anschließend werden die lokalen Namen der E-Mail-Adressen mit dem erzeugten Namensstring der Zielperson vergleichen. Dabei erreicht die lokale Namen \textit{mustermax} eine prozentuale Übereinstimmung von 0,73 \% mit dem Namensstring und \textit{martafrau} 0,27 \%. Da die Prozent-Grenze bei 0,5 \% beträgt, wird die zweite E-Mail-Adresse verworfen.
		
\section{Auswahl der gefundenen Information}
In diesem Abschnitt werden die Methoden zur Auswahl von den gefundenen Informationen beschrieben. Das hat den Grund, dass alle gewonnenen Schlüsselwörter einer Webseite in einer Liste gespeichert sind. Jedoch kann nur eines dieser Schlüsselwörter für die Generierung der Phishing-Mail verwendet werden.
	\subsection{Ziele und Anforderungen}
	Ziel ist es ein Schlüsselwort aus jeweils einer Liste der gefundenen Kategorien zu bestimmen. Dabei soll beachtet werden, wie oft eine Element auf einer Webseite und auf unterschiedlichen Webseiten vorkommt.
	
	\subsection{Lösungsidee}
	Es kann eine Methode entwickelt werden, welche einen Score für die Bewertung eines Elementes berechnet. Hierfür wird eine prozentuale Wertung für das Vorkommen eines Wortes in einer Liste mit der Formel \ref{fml:prozentWort} berechnet, nachdem eine Seite vollständig durchsucht wurde.
	\begin{align}% Mathe-Umgebung, die & ausrichtet
	\label{fml:prozentWort}
	\begin{split}% sorgt dafür, dass das Eingeschlossene sich eine
	% Nummer teilt.
	&\frac{\text{Vorkommen eines Wortes}}{\text{Anzahl aller gefundenen Wörter in der Liste}}\\[0.25\baselineskip]% Gleichung, durch die Länge in []
	\end{split}
	\end{align}
	Die Schlüsselwörter werden anschließend mit dem dazugehörigen Score in einer neuen Liste gespeichert. Jedes Wort kommt dabei nur einmal vor. Eine beispielhafte Liste ist nachstehend dargestellt.
	
	\texttt{[['fussball', 0.7],['basketball', 0,2],['fechten', 0.1]]}
	
	Hierbei ist zu sehen, dass das Wort "'Fußball"' siebenmal öfter als das Wort "'Fechten"' auf der Webseite vorkommt. Solch eine Liste wird für jede durchsuchte Seite erstellt. Nachdem alle Webseiten durchsucht wurden, werden alle erstellten Listen zu einer zusammengefügt. Dabei bleibt die Struktur bestehen, damit erkannt wird, welche Wörter auf unterschiedlichen Webseiten vorkommen. Ein Beispiel hierfür ist die folgende Liste.
	
	\texttt{[[['fussball', 0.7],['basketball', 0.2],['fechten',0.1]],
		[['fussball',\\ 0.5],['volleyball', 0.5]]}
	
	In dieser Liste befinden sich die gewonnenen Informationen aller Webseiten für eine Kategorie. Hier wäre es die Kategorie "'Hobby"'. Für jede dieser kategorisierten Listen muss nun ein Element bestimmt werden, welches am wahrscheinlichsten eine Verbindung zu der Zielperson hat. Dazu wird eine Matrix mit allen Wertungen erstellt. Eine Matrix für das hier verwendetet Beispiel ist in Tabelle \ref{img:beispielMatrix} aufgezeigt. Die Spalten enthalten alle Elemente einmal, die in der Liste vorkommen können. Die Zeilen entsprechen der Anzahl der durchsuchten Webseiten. 
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{ | c | c | c |c |c|}
			\hline
			& Fußball & Basketball & Fechten& Volleyball\\ \hline
			Webseite 1& 0.7 & 0.2 & 0.1 & 0\\ \hline
			Webseite 2& 0.5 & 0 & 0 & 0.5\\ \hline
		\end{tabular}
		\label{img:beispielMatrix}
		\caption{Beispiel-Matrix}
	\end{table}
	
	Mit der nachfolgenden Formel \ref{fml:Score} wird für jedes Element ein endgültiger Score aus der Matrix berechnet. Dafür werden die Spalten summiert und durch die Anzahl der durchsuchten Webseiten geteilt.
	
	\begin{align}% Mathe-Umgebung, die & ausrichtet
	\label{fml:Score}
	\begin{split}% sorgt dafür, dass das Eingeschlossene sich eine
	% Nummer teilt.
	&\sum\limits_{i=0}^{m-1}\sum\limits_{j=0}^{n-1} \frac{liste[j][i][1]}{n}\\[0.25\baselineskip]% Gleichung, durch die Länge in []
	% die Erklärungen evtl. etwas absetzen
	\text{mit } \\
	m &= \text{Anzahl aller möglichen Elemente (Spalten)}\\
	n &= \text{Anzahl der durchsuchten Webseiten (Zeilen)}\\
	i &= \text{Spalte}\\             % dem \text-Makro erzeugen.
	j &= \text{Zeile}
	\end{split}
	\end{align}
	
	
	In dem aufgezeigten Beispiel würde das zu folgendem Ergebnis führen:
	
	\texttt{[['fussball', 0.6],['basketball',0,1],['fechten',0.05],
		['volleyball', 0.25]]}		
	
	Aus dieser Liste kann nun das Schlüsselwort mit der höchsten Wertung gewählt und dem Personenobjekt hinzugefügt werden. In diesem Fall wäre es das Wort "'Fußball"'. Falls zwei Wertungen gleich hoch sind, wird das  Wort, welches als erstes in der entsprechenden Liste vorkommt, ausgewählt.
	
	\subsection{Bewertung der Lösungsidee anhand den Anforderungen}
	Die vorgestellt Methode zur Berechnung eines Scores, bringt alles Anforderungen mit sich. Es wird nicht nur das Vorkommen eines Elements in einer Liste gezählt, sondern berücksichtigt wie oft es auf einer Webseite und auf wie vielen Webseiten es vorkommt. Dadurch können Fehler bei der Gewichtung eines Elements vermieden werden. Das hat den Grund, dass  ein Schlüsselwort auf einer Webseiten öfter auftreten kann. Im Fall dass ein Element auf einer Webseite sehr oft gefunden wird, könnte mit einer Methode bei der ausschließlich das Vorkommen gezählt wird ein Fehler entstehen.  Somit würde ein häufig auftretendes Schlüsselwörter auf einer Webseite andere überstimmen, obwohl diese ebenfalls oft und auf verschiedenen Webseiten zu finden sind. Infolgedessen wird die Methode zur Berechnung eines Scores umgesetzt.
	
\section{Umsetzung der Methode zur Auswahl der gefundenen Information}
\label{subsec:AuswahlInformation}
Zu Beginn wird unterschieden ob ein Score für die Kategorie „Institution“ oder für eine anderer berechnet werden soll. Das hat den Grund, dass Liste bei diesen Kategorien unterschiedlich sortiert werden. Im Fall dass der Score für die Institutionen  ermittel werden soll, wird die Liste mit den gefundenen Elementen der Länge nach sortiert. Somit ist das Element mit der längsten Zeichenkette an erster Stelle. Das hat den Grund, dass bei der Suche nach Institutionen lediglich die Zeichenketten gesucht werden. Somit kann es sein, dass ein langer Institutions-Name unbeabsichtigt einen kürzen beinhaltet. Ein Beispiel hierfür ist die Firma "'MF Musterfirma GmbH \& Co. KG“', welche zusätzlich einen anderen Firmennamen wie "'Musterfirma GmBH"' einschließt . Durch die eben erwähnte Sortierung wird dieser Fehler umgangen. \\
Falls es sich allerdings um eine andere Kategorie handelt, werden die Liste nach der Häufigkeit wie oft ein Element auf einer Webseite vorkommt sortiert. Somit wird bei einem gleichen Score das Element gewählt, welches ein höheres Vorkommen auf einer Webseite hat.\\
Mit der Programmbibliothek  NumPy wird eine Matrix erstellt und mit Nullen befüllt. Anschließend werden die errechneten Scores an den entsprechenden Positionen eingesetzt. Daraufhin werden die Spalten zusammengerechnet und vergleichen. Das Element, welches der Spalte mit dem höchsten Wert entspricht, wird ausgewählt. Im Listing \ref{lst:algoScore} wird der Algorithmus zur Berechnung des finalen Scores und der  Auswahl des entsprechenden Elements dargestellt.\\
\newpage
		\begin{lstlisting}[caption=Algorithmus zur Berechnung des Scores und der Auswahl des Elementes mit dem höchsten Score,label={lst:algoScore}]
			score = 0
			element_with_highest_score = ""
			#every column
			for k in range(0, numpy.size(matrix, 1)):
				#current_score = sum of elements of a column
				current_score = 0
				#every row
				for i in range(0,numpy.size(matrix,0)):
					current_score = current_score + matrix[i][k]
				current_score = current_score / len(list3)
				if current_score > score:
					score = current_score
					element_with_highest_score = instances[k]
		\end{lstlisting}

\section{Kontaktanalyse}
\label{subsec:Kontaktanalyse}
Hier kann die Suche erweitert werden, indem auf soziale und berufliche Verbindungen der Zielperson eingegangen wird. Das heißt, dass bekannte Kontakte der gesuchten Person ebenfalls durchsucht und ausgewertet werden. 	
Durch die erwähnte Methode können weitere Informationen gewonnen werden.

	\subsection{Ziele und Anforderungen}
	Ergänzend zu der Personensuche sollen Kontaktinformationen gefunden werden.  Als Informationsquelle kann jede Social-Media-Plattformen dienen, welche es ermöglicht die Kontakte der gesuchten Person anzuzeigen.  Des Weiteren sollen Informationsquellen ohne  konfigurierbare Sicherheitsvorkehrungen bevorzugt werden, da ansonsten mögliche Kontakte nicht anzeigt werden können.  Um die Funktion der Kontaktanalyse aufzuzeigen ist es ausreichend diese auf einer Social-Media-Webseite durchzuführen.\\
	Eine gefunden Kontaktinformation muss äquivalent zu  einem Attribut  des Opfers sein. Somit kann davon ausgegangen werden, dass beide Personen die selben Interessen in dem Bereich der gefundenen Information haben. 
	
	\subsection{Lösungsideen} 
	Als mögliche Informationsquellen für Kontakte zählen die Social-Media-Seiten von Facebook, LinkedIn, Instagram, Twitter und XING.

	\subsection{Bewertung der Lösungsideen anhand den Anforderungen}
	Diese Methode funktioniert auf der Webseite LinkedIn nicht. Es gibt dort keine Möglichkeit, die Kontakte der gesuchten Person anzuzeigen. Bei Xing kann ein Nutzer einstellen, ob diese Kontaktanzeige freigegeben wird oder nicht. Dadurch sind die Kontakte bei vielen Benutzern nicht erkennbar. Facebook, Twitter und Instagram bieten die Möglichkeit, die Kontakte der gesuchten Person anzuzeigen. Allerdings wird dafür ein Account benötigt.\\
	Somit eignen sich die Seiten Twitter, Xing, Facebook und Instagram. Wie in Kapitel \ref{subsubsec:SocialMediaSeiten} beschrieben, wird kein Facebook-Account angelegt. Dadurch ist es nicht möglich, Kontakte auf dieser Webseite anzuzeigen. Um die Funktion der Methode aufzuzeigen, wird ausschließlich die Webseite Instagram verwendet, da sie alle Anforderungen mit sich bringt und im Ranking der größten sozialen Netzwerke an aktiven Nutzern vor Twitter steht. \cite{RankingSozialeNetzwerke}

\section{Umsetzung der Instagram-Kontaktanalyse} 
Zuallererst wird unterschieden, ob das Profil der gesuchten Person privat oder öffentlich ist. Bei einem öffentlichen Profil können alle Abonnenten und abonnierte Profile angezeigt werden, welche sich unterscheiden. Im Gegensatz dazu werden bei einem privaten Profil, nur eine begrenzte Anzahl von Profilen vorgeschlagen. Des Weiteren kann bei einem privaten Profil nicht unterschieden werden, ob die Abonnenten oder die abonnierten Profile angezeigt werden sollen.\\
Von den gefunden Followern wird jedes einzelne Profil durchsucht, bis eine Übereinstimmung mit der Zielperson gefunden wurde. Eine Übereinstimmung bedeutet, dass auf diesem Profil ein Teil mit dem Opferprofil identisch ist. Ein Beispiel hierfür kann  die selbe Universität oder der selbe Wohnort sein. Sobald dies gefunden wurde, kann die Suche beendet werden. Wenn keine Übereinstimmung der Profile gefunden wurde, wird dem erstellten Opferprofil keine Kontaktformation hinzugefügt.

	\subsection{Auslesen der Kontakte}
	Im ersten Schritt entscheidet der Algorithmus, ob es sich um ein privates oder öffentliches Profil handelt. Dies wird realisiert, indem nach einem String auf der Webseite gesucht wird. Der String lautet "'Dieses Konto ist privat"'. Wenn diese Zeichenfolge gefunden wird, handelt es sich um ein privates Konto. Andernfalls um ein öffentliches.\\
	Damit die Links zu den Kontakt-Profilseiten auf einer privaten Seite herausgelesen werden können, wird ein scrollbarer Container ausgelesen. Dieser Container beinhaltet die vorgeschlagenen Kontakte und zwei Buttons. Wie im Bild \ref{img:instagram_vorschlaege} zu sehen kann mit den beiden Buttons nach rechts und links gewischt werden. Sobald die Links zu den aktuell angezeigten Profilen ausgelesen wurden, wird auf den rechten Button geklickt. Dies wird mit einem vorgetäuschten Mausklick des Selenium WebDrivers realisiert. Durch diese Schritt-für-Schritt-Methode können alle vorgeschlagenen Kontakte ausgelesen werden. Andernfalls werden nur die aktuell angezeigten Profile geladen und gefunden.
	
	
	\begin{figure}[h!]
		\centering
		\includegraphics[ scale=0.3]{bilder/instagram_vorschlaege.png}
		\caption{Container mit Profil-Vorschlägen \cite{instagram}}
		\label{img:instagram_vorschlaege}
	\end{figure}
	
	Falls es sich um ein öffentlich frei zugängliches Profil handelt, kann eine Liste der abonnierten Kontakte angezeigt werden. Diese sind über ein scrollbares Pop-Up-Fenster, wie in Bild \ref{img:instagram_abonniert} dargestellt, einsehbar. Vergleichbar zur Methode bei einer privaten Profilseite wird hier ebenfalls Schritt-für-Schritt durchgescrollt. Dadurch wird jeder Kontakt geladen. Somit können alle Links, welche zu den entsprechenden Profilseiten führen, ausgelesen werden. 
	
	
	
	\begin{figure}[h!]
		\centering
		\fbox{\includegraphics[ scale=0.2]{bilder/instagram_abonniert.png}}
		\caption{Pop-up-Fenster mit abonnierten Profilen \cite{instagram}}
		\label{img:instagram_abonniert}
	\end{figure}
	
	Die Herausforderung besteht darin, dass nicht zu schnell gescrollt werden darf. Andernfalls werden keine weiteren Kontakte geladen. Aus diesem Grund wird ein Algorithmus \ref{lst:AlgoZumDurchscrollen} verwendet, welcher einem menschlichen Verhalten ähneln soll und somit langsam und schrittweise nach unten scrollt. Hierfür wird zuallererst das Pop-up-Fenster gesucht und festgelegt. Anschließend wir die Anzahl der abonnierten Profile gezählt. Die Anzahl der Profile wird dazu verwendet, dass der Algorithmus weiß, wie weit nach unten geblättert werden muss, um alle Profil zu laden.\\
	Im ersten Scroll-Vorgang wird das Fenster nur ein sechstel des möglichen Bereichs nach unten geblättert. Dadurch werden weitere Profile geladen.\\
	In den nächsten Schritte wird das Fenster jeweils ganz nach unten verschoben. Dadurch werden alle Profile geladen. Sobald alle internen Links bekannt sind, wird eine URL zu den entsprechenden Profilseiten erstellt. Diese Seiten werden anschließend wie jede andere Seite ausgelesen und nach Information durchsucht. Infolgedessen wird die gewonnene Information jedes Profils mit der Information der Zielperson verglichen. Die Suche wird bei einem beliebigen Treffer beendet. Anschließend wird die gefundene Information mit dem Namen des Benutzers der Profilseite gespeichert. Diese abgespeicherten Daten können später zur E-Mail-Generierung verwendet werden.\\
	
	\begin{lstlisting}[caption=Algorithmus zum Herunterscrollen des Pop-up Fensters,label={lst:AlgoZumDurchscrollen}]
	# Find the pop-up window
	pop_up = self.browser.find_element_by_class_name("isgrP")        
	# find number of followers
	all_following = int(self.browser.find_element_by_xpath("//li[2]
	/a/span").text)
	# scroll down the page
	for i in range(int(all_following / 6)):
	if i == 0:
	self.browser.execute_script("arguments[0].scrollTop = 
	arguments[0].scrollHeight/5", pop_up)
	time.sleep(2)
	else:
	self.browser.execute_script("arguments[0].scrollTop = 
	arguments[0].scrollHeight", pop_up)
	time.sleep(random.randint(500, 1000) / 1000)
	\end{lstlisting}	

\section{Speicherung der gewonnenen Daten}
	\subsection{Ziele und Anforderungen}
	Die gespeicherten Daten werden von verschiedenen Klassen benötigt. Aus diesem Grund muss es möglich sein, dass andere Klassen auf die Speicherstruktur zugreifen können. Zusätzlich wird eine gute Struktur vorausgesetzt, damit einzelne Attribute der Person ausgewählt werden können. Des Weiteren muss die Speichervariante in Python implementierbar sein und unnötige Lese- und Schreibzugriffe sollen vermieden werden. Es ist nicht notwendig, dass die Daten nach Programmende abrufbar sind. Infolgedessen wird keine externe Speicherung in einer Datenbank oder in einer Datei vorausgesetzt.
	\subsection{Lösungsideen}
	Eine mögliche Speicherung der Daten wäre in einer SQL-Datenbank. Alternativ könnten die Personendaten in einer externen Datei oder mit Hilfe einem Personenobjekt gespeichert werden.
	\subsection{Bewertung der Lösungsideen anhand den Anforderungen}
	Die Umsetzung jeder einzelnen Variante ist mit Python möglich. Für die Verwendung einer SQL-Datenbank spricht die gute Speicherstruktur. Allerdings sind mit einer solchen Datenbank aufwendigere Speicher-und Lesevorgänge verbunden. Die externe Speicherung in einer Datei wie CSV oder TXT ist keine Anforderung. Darüber hinaus müssten diese Dateien verschlüsselt werden, damit sie vor fremden Zugriffen geschützt sind. Dennoch werden die gewonnen Daten in einem Personenobjekt gespeichert. Unnötige Speicher- und Lesezugriffe fallen dadurch weg. Darüber hinaus lässt sich das Personenobjekt einfach an die entsprechenden Klassen übergeben.
\section{Umsetzung der Personenklasse}
Die gewonnenen Daten werden in einem Personenobjekt, wie in Bild \ref{img:personClass} dargestellt, gespeichert. Dabei werden die vom Anwender eingegebenen Daten direkt in das Personenobjekt übertragen. Zusätzlich werden die gefundenen Personenattribute hinzugefügt. Diese sind zu Beginn in Form einer Liste gespeichert. Sobald das häufigste Element mit der Methode \ref{subsec:AuswahlInformation} ermittelt wurde, wird nur das entsprechende Element gespeichert.\\
Zu einem angegebenen Attribut kann eine zusätzliche Information gefunden werden. So ist es denkbar, dass beispielsweise zu einem bekannten Wohnort ein weiterer Ort gefunden wird. Dies ist für die Generierung der Phishing-Mail wichtig. Die Zielperson hat möglicherweise einen höheren Bezug zu dem gefunden Ort. Aus diesem Grund wird bei der Informationsauswahl für die Phishing-Mail die gefundene Information den angegebenen Daten bevorzugt.\\
Falls bei der Kontaktanalyse ein übereinstimmendes Profil gefunden wurde, kann diese Information in dem Attribute "'Gefundene Kontaktinformation"' gespeichert werden. Hierbei wird zuerst der vollständige Kontaktname und anschließend die übereinstimmende Information gespeichert. Ein Beispiel hierfür ist ['Max Mustermann', 'Fußball'].\\\\
	
\begin{figure}[h!]
	\centering
	\includegraphics[ scale=0.3]{bilder/person_class.png}
	\caption{Personenklasse}
	\label{img:personClass}
\end{figure}
	